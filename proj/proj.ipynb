{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = pd.read_csv('anime.csv')\n",
    "rating_df = pd.read_csv('rating.csv')\n",
    "rating_df = rating_df[rating_df['rating'] != -1]  # Remove -1 ratings (unknown)\n",
    "rating_df = rating_df.drop_duplicates(['user_id', 'anime_id'])  # Remove duplicate ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimeRecommender:\n",
    "    def __init__(self, rating_df, anime_df):\n",
    "        self.rating_df = rating_df\n",
    "        self.anime_df = anime_df\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess the data to improve recommendation quality\"\"\"\n",
    "        # Filter users who have rated a minimum number of anime\n",
    "        user_counts = self.rating_df['user_id'].value_counts()\n",
    "        valid_users = user_counts[user_counts >= 5].index\n",
    "        \n",
    "        # Filter anime that have received a minimum number of ratings\n",
    "        anime_counts = self.rating_df['anime_id'].value_counts()\n",
    "        valid_anime = anime_counts[anime_counts >= 10].index\n",
    "        \n",
    "        # Filter the ratings dataframe\n",
    "        filtered_ratings = self.rating_df[\n",
    "            (self.rating_df['user_id'].isin(valid_users)) & \n",
    "            (self.rating_df['anime_id'].isin(valid_anime))\n",
    "        ]\n",
    "        \n",
    "        # Normalize ratings per user\n",
    "        user_mean = filtered_ratings.groupby('user_id')['rating'].transform('mean')\n",
    "        user_std = filtered_ratings.groupby('user_id')['rating'].transform('std')\n",
    "        filtered_ratings['normalized_rating'] = (filtered_ratings['rating'] - user_mean) / user_std.fillna(1)\n",
    "        \n",
    "        return filtered_ratings\n",
    "        \n",
    "    def train(self, train_data):\n",
    "        \"\"\"Train the recommender system using only training data\"\"\"\n",
    "        self.train_df = train_data\n",
    "        \n",
    "        # Build matrix using normalized ratings\n",
    "        self.user_anime_matrix = self.train_df.pivot(\n",
    "            index='user_id',\n",
    "            columns='anime_id',\n",
    "            values='normalized_rating'\n",
    "        )\n",
    "        \n",
    "        # Fill NaN with 0 after pivoting\n",
    "        self.user_anime_matrix = self.user_anime_matrix.fillna(0)\n",
    "        \n",
    "        # Convert to sparse matrix\n",
    "        self.user_anime_sparse = csr_matrix(self.user_anime_matrix.values)\n",
    "        \n",
    "        # Fit the nearest neighbors model with more neighbors\n",
    "        self.model = NearestNeighbors(\n",
    "            metric='cosine',\n",
    "            algorithm='brute',\n",
    "            n_neighbors=20  # Increased from 10\n",
    "        )\n",
    "        self.model.fit(self.user_anime_sparse)\n",
    "        \n",
    "        # Store user means for denormalization\n",
    "        self.user_means = self.train_df.groupby('user_id')['rating'].mean()\n",
    "        self.user_stds = self.train_df.groupby('user_id')['rating'].std()\n",
    "    \n",
    "    def predict_rating(self, user_id, anime_id):\n",
    "        \"\"\"Predict rating for a given user-anime pair\"\"\"\n",
    "        try:\n",
    "            # Find similar users\n",
    "            user_index = self.user_anime_matrix.index.get_loc(user_id)\n",
    "            user_vector = self.user_anime_sparse[user_index]\n",
    "            distances, indices = self.model.kneighbors(user_vector.reshape(1, -1))\n",
    "            \n",
    "            # Convert distances to weights\n",
    "            weights = 1 / (distances.flatten() + 1e-6)\n",
    "            \n",
    "            # Get similar users\n",
    "            similar_users = self.user_anime_matrix.index[indices.flatten()]\n",
    "            \n",
    "            # Get ratings from similar users for the target anime\n",
    "            similar_ratings = self.train_df[\n",
    "                (self.train_df['user_id'].isin(similar_users)) & \n",
    "                (self.train_df['anime_id'] == anime_id)\n",
    "            ]\n",
    "            \n",
    "            if len(similar_ratings) == 0:\n",
    "                return self.user_means[user_id]\n",
    "            \n",
    "            # Calculate weighted average rating\n",
    "            weighted_sum = 0\n",
    "            weight_sum = 0\n",
    "            \n",
    "            for idx, rating in enumerate(similar_ratings['rating']):\n",
    "                weighted_sum += rating * weights[idx]\n",
    "                weight_sum += weights[idx]\n",
    "            \n",
    "            if weight_sum == 0:\n",
    "                return self.user_means[user_id]\n",
    "                \n",
    "            predicted_rating = weighted_sum / weight_sum\n",
    "            \n",
    "            # Clip predictions to valid range\n",
    "            return np.clip(predicted_rating, 1, 10)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Return user's mean rating if prediction fails\n",
    "            return self.user_means.get(user_id, 5)\n",
    "\n",
    "def evaluate_recommender(rating_df, anime_df, sample_size=20000, test_size=0.33, random_state=42):\n",
    "    # Initialize recommender\n",
    "    recommender = AnimeRecommender(rating_df, anime_df)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    processed_ratings = recommender.preprocess_data()\n",
    "    \n",
    "    # Take a sample if specified\n",
    "    if sample_size and sample_size < len(processed_ratings):\n",
    "        processed_ratings = processed_ratings.sample(sample_size, random_state=random_state)\n",
    "    \n",
    "    # Split the data\n",
    "    train_df, test_df = train_test_split(\n",
    "        processed_ratings,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Train the recommender\n",
    "    recommender.train(train_df)\n",
    "    \n",
    "    # Predict ratings for the test set\n",
    "    print(\"Evaluating recommendations...\")\n",
    "    tqdm.pandas()\n",
    "    test_df['predicted_rating'] = test_df.progress_apply(\n",
    "        lambda row: recommender.predict_rating(row['user_id'], row['anime_id']),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(test_df['rating'], test_df['predicted_rating'])\n",
    "    rmse = np.sqrt(np.mean((test_df['rating'] - test_df['predicted_rating'])**2))\n",
    "    \n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")\n",
    "    \n",
    "    return recommender, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridAnimeRecommender:\n",
    "    def __init__(self, rating_df, anime_df):\n",
    "        self.rating_df = rating_df\n",
    "        self.anime_df = anime_df\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess the data to improve recommendation quality\"\"\"\n",
    "        # Increase minimum ratings thresholds for better reliability\n",
    "        user_counts = self.rating_df['user_id'].value_counts()\n",
    "        valid_users = user_counts[user_counts >= 10].index  # Increased from 5\n",
    "        \n",
    "        anime_counts = self.rating_df['anime_id'].value_counts()\n",
    "        valid_anime = anime_counts[anime_counts >= 20].index  # Increased from 10\n",
    "        \n",
    "        # Filter the ratings dataframe\n",
    "        filtered_ratings = self.rating_df[\n",
    "            (self.rating_df['user_id'].isin(valid_users)) & \n",
    "            (self.rating_df['anime_id'].isin(valid_anime))\n",
    "        ]\n",
    "        \n",
    "        # Enhanced normalization with outlier handling\n",
    "        user_mean = filtered_ratings.groupby('user_id')['rating'].transform('mean')\n",
    "        user_std = filtered_ratings.groupby('user_id')['rating'].transform('std')\n",
    "        \n",
    "        # Handle zero standard deviation case more gracefully\n",
    "        user_std = user_std.replace(0, 1)\n",
    "        \n",
    "        # Clip normalized ratings to reduce impact of extreme values\n",
    "        filtered_ratings['normalized_rating'] = np.clip(\n",
    "            (filtered_ratings['rating'] - user_mean) / user_std,\n",
    "            -3,  # Lower bound: 3 standard deviations\n",
    "            3    # Upper bound: 3 standard deviations\n",
    "        )\n",
    "        \n",
    "        return filtered_ratings\n",
    "    \n",
    "    def train(self, train_data):\n",
    "        \"\"\"Train the recommender system using only training data\"\"\"\n",
    "        self.train_df = train_data\n",
    "        \n",
    "        # Build user-anime matrix using normalized ratings\n",
    "        self.user_anime_matrix = self.train_df.pivot(\n",
    "            index='user_id',\n",
    "            columns='anime_id',\n",
    "            values='normalized_rating'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        # Apply TruncatedSVD for dimensionality reduction\n",
    "        n_components = min(50, min(self.user_anime_matrix.shape) - 1)\n",
    "        self.svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "        user_anime_reduced = self.svd.fit_transform(self.user_anime_matrix)\n",
    "        \n",
    "        self.user_anime_sparse = csr_matrix(user_anime_reduced)\n",
    "        \n",
    "        self.model = NearestNeighbors(\n",
    "            metric='cosine',\n",
    "            algorithm='brute',\n",
    "            n_neighbors=30 \n",
    "        )\n",
    "        self.model.fit(self.user_anime_sparse)\n",
    "        \n",
    "        # Store user statistics\n",
    "        self.user_means = self.train_df.groupby('user_id')['rating'].mean()\n",
    "        self.user_stds = self.train_df.groupby('user_id')['rating'].std().fillna(1)\n",
    "        \n",
    "        # Enhanced genre processing\n",
    "        self.anime_df['genre'] = self.anime_df['genre'].fillna('')\n",
    "        self.vectorizer = TfidfVectorizer(  # Changed from CountVectorizer\n",
    "            tokenizer=lambda x: x.split(', '),\n",
    "            min_df=2  # Ignore very rare genres\n",
    "        )\n",
    "        self.genre_matrix = self.vectorizer.fit_transform(self.anime_df['genre'])\n",
    "        \n",
    "        # Calculate global statistics\n",
    "        self.global_mean = self.train_df['rating'].mean()\n",
    "        self.global_std = self.train_df['rating'].std()\n",
    "    \n",
    "    def predict_rating(self, user_id, anime_id, genre_weight=0.3):  # Adjusted default weight\n",
    "        \"\"\"Predict rating using enhanced hybrid approach\"\"\"\n",
    "        try:\n",
    "            # 1. Collaborative Filtering Component\n",
    "            user_index = self.user_anime_matrix.index.get_loc(user_id)\n",
    "            user_vector = self.user_anime_sparse[user_index]\n",
    "            distances, indices = self.model.kneighbors(user_vector.reshape(1, -1))\n",
    "            \n",
    "            # Enhanced distance-to-weight conversion\n",
    "            weights = np.exp(-distances.flatten())  # Exponential decay\n",
    "            similar_users = self.user_anime_matrix.index[indices.flatten()]\n",
    "            \n",
    "            similar_ratings = self.train_df[\n",
    "                (self.train_df['user_id'].isin(similar_users)) & \n",
    "                (self.train_df['anime_id'] == anime_id)\n",
    "            ]\n",
    "            \n",
    "            if len(similar_ratings) > 0:\n",
    "                weighted_sum = np.sum(weights * similar_ratings['rating'].values)\n",
    "                weight_sum = np.sum(weights)\n",
    "                cf_rating = weighted_sum / weight_sum if weight_sum > 0 else self.user_means.get(user_id, self.global_mean)\n",
    "            else:\n",
    "                cf_rating = self.user_means.get(user_id, self.global_mean)\n",
    "            \n",
    "            # 2 Genre-Based Component\n",
    "            anime_idx = self.anime_df[self.anime_df['anime_id'] == anime_id].index[0]\n",
    "            user_rated_anime = self.train_df[self.train_df['user_id'] == user_id]\n",
    "            \n",
    "            if not user_rated_anime.empty:\n",
    "                genre_similarities = []\n",
    "                genre_ratings = []\n",
    "                \n",
    "                for _, row in user_rated_anime.iterrows():\n",
    "                    rated_anime_idx = self.anime_df[self.anime_df['anime_id'] == row['anime_id']].index[0]\n",
    "                    similarity = cosine_similarity(\n",
    "                        self.genre_matrix[anime_idx],\n",
    "                        self.genre_matrix[rated_anime_idx]\n",
    "                    )[0, 0]\n",
    "                    \n",
    "                    \n",
    "                    genre_similarities.append(similarity)\n",
    "                    genre_ratings.append(row['rating'])\n",
    "                \n",
    "                # Use top-k most similar items only\n",
    "                top_k = 10\n",
    "                if len(genre_similarities) > top_k:\n",
    "                    top_indices = np.argsort(genre_similarities)[-top_k:]\n",
    "                    genre_similarities = np.array(genre_similarities)[top_indices]\n",
    "                    genre_ratings = np.array(genre_ratings)[top_indices]\n",
    "                \n",
    "                genre_rating = np.average(genre_ratings, weights=genre_similarities)\n",
    "            else:\n",
    "                genre_rating = self.global_mean\n",
    "            \n",
    "            # 3. Enhanced prediction combination\n",
    "            confidence_cf = len(similar_ratings) / 10  # Scale factor for CF confidence\n",
    "            confidence_cb = len(genre_similarities) if 'genre_similarities' in locals() else 0\n",
    "            confidence_cb = confidence_cb / 10  # Scale factor for CB confidence\n",
    "            \n",
    "            # Adjust weights based on confidence\n",
    "            total_confidence = confidence_cf + confidence_cb\n",
    "            if total_confidence > 0:\n",
    "                cf_weight = (1 - genre_weight) * (confidence_cf / total_confidence)\n",
    "                cb_weight = genre_weight * (confidence_cb / total_confidence)\n",
    "            else:\n",
    "                cf_weight = 1 - genre_weight\n",
    "                cb_weight = genre_weight\n",
    "            \n",
    "            predicted_rating = (cf_weight * cf_rating + cb_weight * genre_rating)\n",
    "            \n",
    "            # Regression to the mean for low-confidence predictions\n",
    "            confidence = (confidence_cf + confidence_cb) / 2\n",
    "            predicted_rating = (confidence * predicted_rating + \n",
    "                              (1 - confidence) * self.global_mean)\n",
    "            \n",
    "            return np.clip(predicted_rating, 1, 10)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self.user_means.get(user_id, self.global_mean)\n",
    "\n",
    "def evaluate_hybrid_recommender(rating_df, anime_df, sample_size=20000, test_size=0.33, genre_weight=1, random_state=42):\n",
    "    # Initialize recommender\n",
    "    recommender = HybridAnimeRecommender(rating_df, anime_df)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    processed_ratings = recommender.preprocess_data()\n",
    "    \n",
    "    # Take a sample if specified\n",
    "    if sample_size and sample_size < len(processed_ratings):\n",
    "        processed_ratings = processed_ratings.sample(sample_size, random_state=random_state)\n",
    "    \n",
    "    # Split the data\n",
    "    train_df, test_df = train_test_split(\n",
    "        processed_ratings,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Train the recommender\n",
    "    recommender.train(train_df)\n",
    "    \n",
    "    # Predict ratings for the test set\n",
    "    print(\"Evaluating hybrid recommendations...\")\n",
    "    tqdm.pandas()\n",
    "    test_df['predicted_rating'] = test_df.progress_apply(\n",
    "        lambda row: recommender.predict_rating(\n",
    "            row['user_id'], \n",
    "            row['anime_id'], \n",
    "            genre_weight=genre_weight\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(test_df['rating'], test_df['predicted_rating'])\n",
    "    rmse = np.sqrt(np.mean((test_df['rating'] - test_df['predicted_rating'])**2))\n",
    "    \n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Root Mean Square Error (RMSE): {rmse:.4f}\")\n",
    "    \n",
    "    return recommender, mae, rmse\n",
    "\n",
    "# Example usage\n",
    "def compare_recommenders(rating_df, anime_df, sample_size=10000):\n",
    "    print(\"Evaluating Collaborative Filtering Recommender:\")\n",
    "    cf_recommender, cf_mae, cf_rmse = evaluate_recommender(rating_df, anime_df, sample_size=sample_size)\n",
    "    \n",
    "    print(\"\\nEvaluating Hybrid Recommender:\")\n",
    "    hybrid_recommender, hybrid_mae, hybrid_rmse = evaluate_hybrid_recommender(rating_df, anime_df, sample_size=sample_size)\n",
    "    \n",
    "    print(\"\\nComparison Summary:\")\n",
    "    print(f\"{'Method':<25} {'MAE':<10} {'RMSE':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    print(f\"{'Collaborative Filtering':<25} {cf_mae:<10.4f} {cf_rmse:<10.4f}\")\n",
    "    print(f\"{'Hybrid Approach':<25} {hybrid_mae:<10.4f} {hybrid_rmse:<10.4f}\")\n",
    "    \n",
    "    return cf_recommender, hybrid_recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Collaborative Filtering Recommender:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/dhv__9sj0yv1kz0pfv2ds9nh0000gn/T/ipykernel_27212/3772738179.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_ratings['normalized_rating'] = (filtered_ratings['rating'] - user_mean) / user_std.fillna(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recommendations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33000/33000 [00:44<00:00, 738.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 1.6500\n",
      "Root Mean Square Error (RMSE): 2.1711\n",
      "\n",
      "Evaluating Hybrid Recommender:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/dhv__9sj0yv1kz0pfv2ds9nh0000gn/T/ipykernel_27212/97365057.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_ratings['normalized_rating'] = np.clip(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating hybrid recommendations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33000/33000 [03:53<00:00, 141.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 1.2043\n",
      "Root Mean Square Error (RMSE): 1.5479\n",
      "\n",
      "Comparison Summary:\n",
      "Method                    MAE        RMSE      \n",
      "---------------------------------------------\n",
      "Collaborative Filtering   1.6500     2.1711    \n",
      "Hybrid Approach           1.2043     1.5479    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cf_recommender, hybrid_recommender = compare_recommenders(rating_df, anime_df, sample_size=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
