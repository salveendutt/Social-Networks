{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 203451 tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/dhv__9sj0yv1kz0pfv2ds9nh0000gn/T/ipykernel_31992/407309783.py:118: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig('twitter_network.png', dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Check twitter_network.png for visualization and twitter_analysis_report.txt for the report.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "def load_twitter_data(url):\n",
    "    \"\"\"\n",
    "    Load Twitter data with proper parsing of list fields\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        # Convert string representations of lists to actual lists\n",
    "        df['hashtags'] = df['hashtags'].apply(ast.literal_eval)\n",
    "        df['mentions'] = df['mentions'].apply(ast.literal_eval)\n",
    "        df['expanded_urls'] = df['expanded_urls'].apply(ast.literal_eval)\n",
    "        print(f\"Loaded {len(df)} tweets\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_interaction_network(df):\n",
    "    \"\"\"\n",
    "    Create a network based on mentions, retweets, and replies\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        source_user = row['user_key']\n",
    "        \n",
    "        # Add source user node\n",
    "        if not G.has_node(source_user):\n",
    "            G.add_node(source_user)\n",
    "        \n",
    "        # Add edges for mentions\n",
    "        for mentioned_user in row['mentions']:\n",
    "            if not G.has_node(mentioned_user):\n",
    "                G.add_node(mentioned_user)\n",
    "            G.add_edge(source_user, mentioned_user, type='mention')\n",
    "        \n",
    "        # Add edges for replies\n",
    "        if pd.notna(row['in_reply_to_status_id']) and row['in_reply_to_status_id'] != \"\":\n",
    "            reply_to = df[df['tweet_id'] == row['in_reply_to_status_id']]['user_key'].values\n",
    "            if len(reply_to) > 0:\n",
    "                if not G.has_node(reply_to[0]):\n",
    "                    G.add_node(reply_to[0])\n",
    "                G.add_edge(source_user, reply_to[0], type='reply')\n",
    "        \n",
    "        # Add edges for retweets\n",
    "        if pd.notna(row['retweeted_status_id']) and row['retweeted_status_id'] != \"\":\n",
    "            retweeted = df[df['tweet_id'] == row['retweeted_status_id']]['user_key'].values\n",
    "            if len(retweeted) > 0:\n",
    "                if not G.has_node(retweeted[0]):\n",
    "                    G.add_node(retweeted[0])\n",
    "                G.add_edge(source_user, retweeted[0], type='retweet')\n",
    "    \n",
    "    return G\n",
    "\n",
    "def analyze_network(G):\n",
    "    \"\"\"\n",
    "    Perform network analysis and return key metrics\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'num_nodes': G.number_of_nodes(),\n",
    "        'num_edges': G.number_of_edges(),\n",
    "        'density': nx.density(G),\n",
    "        'avg_degree': sum(dict(G.degree()).values()) / G.number_of_nodes(),\n",
    "    }\n",
    "    \n",
    "    # Calculate various centrality metrics\n",
    "    metrics['degree_cent'] = nx.degree_centrality(G)\n",
    "    metrics['in_degree_cent'] = nx.in_degree_centrality(G)\n",
    "    metrics['out_degree_cent'] = nx.out_degree_centrality(G)\n",
    "    \n",
    "    # Find top users by different metrics\n",
    "    metrics['top_influential'] = sorted(metrics['degree_cent'].items(), \n",
    "                                      key=lambda x: x[1], reverse=True)[:10]\n",
    "    metrics['top_mentioned'] = sorted(metrics['in_degree_cent'].items(), \n",
    "                                    key=lambda x: x[1], reverse=True)[:10]\n",
    "    metrics['top_active'] = sorted(metrics['out_degree_cent'].items(), \n",
    "                                 key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_network(G, metrics, title=\"Twitter Interaction Network\"):\n",
    "    \"\"\"\n",
    "    Create and save network visualization with different edge colors for different types\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Use spring layout for visualization\n",
    "    pos = nx.spring_layout(G, k=1.5, iterations=50)\n",
    "    \n",
    "    # Draw nodes\n",
    "    node_sizes = [3000 * metrics['degree_cent'][node] for node in G.nodes()]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', \n",
    "                          node_size=node_sizes, alpha=0.7)\n",
    "    \n",
    "    # Draw edges with different colors based on type\n",
    "    edge_colors = {'mention': 'blue', 'reply': 'green', 'retweet': 'red'}\n",
    "    \n",
    "    for edge_type, color in edge_colors.items():\n",
    "        edges = [(u, v) for (u, v, d) in G.edges(data=True) \n",
    "                if d.get('type') == edge_type]\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color=color, \n",
    "                             alpha=0.4, arrows=True, arrowsize=10)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.plot([], [], 'b-', label='Mention')\n",
    "    plt.plot([], [], 'g-', label='Reply')\n",
    "    plt.plot([], [], 'r-', label='Retweet')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('twitter_network.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_tweet_patterns(df):\n",
    "    \"\"\"\n",
    "    Analyze patterns in the tweet dataset\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        'total_tweets': len(df),\n",
    "        'unique_users': df['user_key'].nunique(),\n",
    "        'hashtag_stats': {},\n",
    "        'temporal_patterns': {}\n",
    "    }\n",
    "    \n",
    "    # Analyze hashtags\n",
    "    all_hashtags = [tag for tags in df['hashtags'] for tag in tags]\n",
    "    patterns['hashtag_stats'] = {\n",
    "        'total_hashtags': len(all_hashtags),\n",
    "        'unique_hashtags': len(set(all_hashtags)),\n",
    "        'top_hashtags': pd.Series(all_hashtags).value_counts().head(10).to_dict()\n",
    "    }\n",
    "    \n",
    "    # Temporal analysis\n",
    "    df['created_at'] = pd.to_datetime(df['created_str'])\n",
    "    patterns['temporal_patterns'] = {\n",
    "        'tweets_by_hour': df.groupby(df['created_at'].dt.hour).size().to_dict(),\n",
    "        'tweets_by_day': df.groupby(df['created_at'].dt.dayofweek).size().to_dict()\n",
    "    }\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def generate_report(df, metrics, patterns):\n",
    "    \"\"\"\n",
    "    Generate comprehensive analysis report\n",
    "    \"\"\"\n",
    "    report = f\"\"\"\n",
    "Twitter Network Analysis Report\n",
    "=============================\n",
    "\n",
    "Network Statistics:\n",
    "-----------------\n",
    "• Total Users in Network: {metrics['num_nodes']:,}\n",
    "• Total Interactions: {metrics['num_edges']:,}\n",
    "• Network Density: {metrics['density']:.4f}\n",
    "• Average Interactions per User: {metrics['avg_degree']:.2f}\n",
    "\n",
    "Top Influential Users (by Total Interactions):\n",
    "------------------------------------------\n",
    "\"\"\"\n",
    "    for user, score in metrics['top_influential']:\n",
    "        report += f\"• {user}: {score:.4f}\\n\"\n",
    "    \n",
    "    report += \"\\nMost Mentioned Users:\\n\"\n",
    "    for user, score in metrics['top_mentioned']:\n",
    "        report += f\"• {user}: {score:.4f}\\n\"\n",
    "    \n",
    "    report += \"\\nMost Active Users:\\n\"\n",
    "    for user, score in metrics['top_active']:\n",
    "        report += f\"• {user}: {score:.4f}\\n\"\n",
    "    \n",
    "    report += \"\\nHashtag Analysis:\\n\"\n",
    "    report += f\"• Total Hashtags Used: {patterns['hashtag_stats']['total_hashtags']:,}\\n\"\n",
    "    report += f\"• Unique Hashtags: {patterns['hashtag_stats']['unique_hashtags']:,}\\n\"\n",
    "    report += \"\\nTop Hashtags:\\n\"\n",
    "    for hashtag, count in patterns['hashtag_stats']['top_hashtags'].items():\n",
    "        report += f\"• #{hashtag}: {count:,} uses\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "def main():\n",
    "    # URL to the dataset\n",
    "    url = \"https://nodeassets.nbcnews.com/russian-twitter-trolls/tweets.csv\"\n",
    "    \n",
    "    # Load data\n",
    "    df = load_twitter_data(url)\n",
    "    \n",
    "    if df is not None:\n",
    "        # Create and analyze network\n",
    "        G = create_interaction_network(df)\n",
    "        metrics = analyze_network(G)\n",
    "        patterns = analyze_tweet_patterns(df)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        visualize_network(G, metrics)\n",
    "        \n",
    "        # Generate report\n",
    "        report = generate_report(df, metrics, patterns)\n",
    "        \n",
    "        # Save report\n",
    "        with open('twitter_analysis_report.txt', 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        print(\"Analysis complete. Check twitter_network.png for visualization and twitter_analysis_report.txt for the report.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
